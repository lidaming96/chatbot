from openai import OpenAI
import streamlit as st
from langchain_community.llms.ollama import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain

#with st.sidebar:
#    #openai_api_key = st.text_input("OpenAI API Key", key="chatbot_api_key", type="password")
#    #"[Get an OpenAI API key](https://platform.openai.com/account/api-keys)"
#    deepseek_api_key = st.text_input("DeepSeek API Key", key="chatbot_api_key", type="password")
#    "[Get a DeepSeek API key](https://platform.deepseek.com/user/apikeys)"  # æ›´æ–°ä¸ºDeepSeekè·å–é“¾æ¥
#    "[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)"
#    "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)"
def get_local_models():
    try:
        from ollama import list
        models = list()
        return [model['name'] for model in models['models']]
    except:
        return ["qwen3:1.7b"]  # é»˜è®¤æ¨¡å‹

with st.sidebar:
    st.header("å‚æ•°é…ç½®") # ä¾§è¾¹æ åç§°
    selected_model = st.selectbox("é€‰æ‹©æœ¬åœ°å¤§æ¨¡å‹:", get_local_models())
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡æ¡£:", type=["txt", "pdf"])



st.title("ğŸ’¬ Chatbot with Rag")
#st.caption("ğŸš€ A Streamlit chatbot powered by OpenAI")
st.caption("ğŸš€ A Streamlit chatbot powered by DeepSeek")  # æ›´æ–°è¯´æ˜æ–‡å­—
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None


if uploaded_file is not None and st.session_state.rag_chain is None:
    with st.spinner("æ–‡æ¡£å¤„ç†ä¸­..."):
        # 1ï¼Œè¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ£€æŸ¥ä¸Šä¼ çš„æ–‡ä»¶æ˜¯å¦ä¸ºPDFæ ¼å¼
        if uploaded_file.type == "application/pdf":
            # å¯¼å…¥ PyPDF2 åº“ä¸­çš„ PdfReader ç±»ï¼Œç”¨äºè¯»å– PDF æ–‡ä»¶
            from PyPDF2 import PdfReader
            pdf_reader = PdfReader(uploaded_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text()
        else:
            text = uploaded_file.getvalue().decode("utf-8")

        # 2ï¼Œåˆ†å‰²æ–‡æœ¬
        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
        texts = text_splitter.split_text(text)

        # 3ï¼Œæ–‡æœ¬å‘é‡åŒ– + 4ï¼Œæ–‡æœ¬å‘é‡å­˜å‚¨
        embeddings = HuggingFaceEmbeddings()
        vectorstore = FAISS.from_texts(texts, embeddings)

        # åˆå§‹åŒ–Ollamaæ¨¡å‹
        # è¿™é‡Œåˆå§‹åŒ–äº†ä¸€ä¸ªå›è°ƒç®¡ç†å™¨ï¼Œå¹¶æ³¨å†Œäº†ä¸€ä¸ªæµå¼è¾“å‡ºå›è°ƒå¤„ç†å™¨ã€‚
        # è¿™ä¸ªå¤„ç†å™¨ä¼šåœ¨æ¨¡å‹ç”Ÿæˆå“åº”æ—¶ï¼Œå®æ—¶åœ°å°†ç”Ÿæˆçš„å†…å®¹è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºï¼ˆé€šå¸¸æ˜¯æ§åˆ¶å°ï¼‰ã€‚
        llm = Ollama(
            model=selected_model,
            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
        )

        # åˆ›å»ºRAGé“¾
        st.session_state.rag_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(),
            return_source_documents=True,
        )
    st.success("æ–‡æ¡£å¤„ç†å®Œæˆï¼Œå¯ä»¥ä½¿ç”¨RAGäº†")

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    #if not openai_api_key:
    #st.info("Please add your OpenAI API key to continue.")
    ##if not deepseek_api_key:  # æ ¡éªŒDeepSeekå¯†é’¥
    ##    st.info("Please add your DeepSeek API key to continue.")
    ##    st.stop()

    client = OpenAI(
        #api_key=openai_api_key
        ##api_key=deepseek_api_key,
        api_key='sk-d3c9e1f7573242c0b1ad62e2f309310d',
        base_url="https://api.deepseek.com/v1",  # æ·»åŠ DeepSeekä¸“ç”¨URL
    )
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    # ç”ŸæˆAIå“åº”
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        if st.session_state.rag_chain:
            # ä½¿ç”¨RAGé“¾ç”Ÿæˆå“åº”
            chat_history = [(m["content"], "") for m in st.session_state.messages[:-1] if m["role"] == "user"]
            response = st.session_state.rag_chain({"question": prompt, "chat_history": chat_history})
            full_response = response["answer"]

            # æ˜¾ç¤ºæºæ–‡æ¡£
            st.write("Sources:")
            for doc in response["source_documents"]:
                st.write(doc.page_content[:100] + "...")
        else:
            # ä½¿ç”¨æ™®é€šOllamaæ¨¡å‹ç”Ÿæˆå“åº”
            llm = Ollama(
                model=selected_model,
                callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
            )
            for chunk in llm.stream(prompt):
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)

    # æ·»åŠ AIå“åº”åˆ°èŠå¤©å†å²
    st.session_state.messages.append({"role": "assistant", "content": full_response})
